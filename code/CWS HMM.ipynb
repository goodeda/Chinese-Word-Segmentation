{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9984719e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T22:15:30.445936Z",
     "start_time": "2021-12-23T22:15:30.438419Z"
    }
   },
   "source": [
    "## Chinese Word Segmentation (III)--HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf3a37",
   "metadata": {},
   "source": [
    "### As previous three approaches are all unable to discover new words, new method should be introduced. HMM (Hidden Markov Chain) can deal with unseen words/ words out of vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4a89e",
   "metadata": {},
   "source": [
    "### We assume each observation has a hidden state indicating if we should divide it or combine it with other character. Still take \"赫尔辛基大学在芬兰\"(The university of Helsinki is in Finland/Suomessa on Helsigin yliopisto) as example, so the segmentation can be 赫尔辛基 / 大学 / 在 / 芬兰 The effect would be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9769c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T21:39:04.946484Z",
     "start_time": "2021-12-23T21:39:04.804424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Generated by graphviz version 2.49.3 (20211023.0002)\r\n -->\r\n<!-- Pages: 1 -->\r\n<svg width=\"432pt\" height=\"182pt\"\r\n viewBox=\"0.00 0.00 432.00 182.15\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.55 0.55) rotate(0) translate(4 327.1)\">\r\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-327.1 781.26,-327.1 781.26,4 -4,4\"/>\r\n<!-- B -->\r\n<g id=\"node1\" class=\"node\">\r\n<title>B</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-271\" rx=\"18\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"18\" y=\"-267.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B</text>\r\n</g>\r\n<!-- 赫 -->\r\n<g id=\"node2\" class=\"node\">\r\n<title>赫</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"94.1\" cy=\"-301\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"94.1\" y=\"-297.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">赫</text>\r\n</g>\r\n<!-- B&#45;&gt;赫 -->\r\n<g id=\"edge1\" class=\"edge\">\r\n<title>B&#45;&gt;赫</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M35.08,-277.51C43.51,-280.92 54.09,-285.21 63.91,-289.18\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"62.77,-292.5 73.36,-293.01 65.4,-286.01 62.77,-292.5\"/>\r\n</g>\r\n<!-- M -->\r\n<g id=\"node3\" class=\"node\">\r\n<title>M</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"94.1\" cy=\"-242\" rx=\"18.7\" ry=\"18.7\"/>\r\n<text text-anchor=\"middle\" x=\"94.1\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">M</text>\r\n</g>\r\n<!-- B&#45;&gt;M -->\r\n<g id=\"edge10\" class=\"edge\">\r\n<title>B&#45;&gt;M</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M35.08,-264.71C44.3,-261.09 56.11,-256.47 66.67,-252.34\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"68.24,-255.48 76.28,-248.58 65.69,-248.97 68.24,-255.48\"/>\r\n</g>\r\n<!-- 尔 -->\r\n<g id=\"node4\" class=\"node\">\r\n<title>尔</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"175.59\" cy=\"-274\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"175.59\" y=\"-270.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">尔</text>\r\n</g>\r\n<!-- M&#45;&gt;尔 -->\r\n<g id=\"edge2\" class=\"edge\">\r\n<title>M&#45;&gt;尔</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M111.96,-248.78C121.69,-252.7 134.19,-257.73 145.47,-262.27\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"144.37,-265.61 154.96,-266.09 146.99,-259.11 144.37,-265.61\"/>\r\n</g>\r\n<!-- M_ -->\r\n<g id=\"node5\" class=\"node\">\r\n<title>M_</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"175.59\" cy=\"-211\" rx=\"23.3\" ry=\"23.3\"/>\r\n<text text-anchor=\"middle\" x=\"175.59\" y=\"-207.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">M_</text>\r\n</g>\r\n<!-- M&#45;&gt;M_ -->\r\n<g id=\"edge11\" class=\"edge\">\r\n<title>M&#45;&gt;M_</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M111.96,-235.42C121.24,-231.8 133.05,-227.2 143.9,-222.97\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"145.46,-226.12 153.51,-219.22 142.92,-219.59 145.46,-226.12\"/>\r\n</g>\r\n<!-- 辛 -->\r\n<g id=\"node6\" class=\"node\">\r\n<title>辛</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"257.09\" cy=\"-240\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"257.09\" y=\"-236.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">辛</text>\r\n</g>\r\n<!-- M_&#45;&gt;辛 -->\r\n<g id=\"edge3\" class=\"edge\">\r\n<title>M_&#45;&gt;辛</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M197.9,-218.77C206.7,-221.98 217.06,-225.76 226.58,-229.23\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"225.5,-232.56 236.1,-232.7 227.9,-225.99 225.5,-232.56\"/>\r\n</g>\r\n<!-- E -->\r\n<g id=\"node7\" class=\"node\">\r\n<title>E</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"257.09\" cy=\"-182\" rx=\"18\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"257.09\" y=\"-178.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">E</text>\r\n</g>\r\n<!-- M_&#45;&gt;E -->\r\n<g id=\"edge12\" class=\"edge\">\r\n<title>M_&#45;&gt;E</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M197.9,-203.23C207.94,-199.56 219.99,-195.17 230.52,-191.33\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"231.83,-194.57 240.03,-187.86 229.43,-188 231.83,-194.57\"/>\r\n</g>\r\n<!-- 基 -->\r\n<g id=\"node8\" class=\"node\">\r\n<title>基</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"337.29\" cy=\"-213\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"337.29\" y=\"-209.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">基</text>\r\n</g>\r\n<!-- E&#45;&gt;基 -->\r\n<g id=\"edge4\" class=\"edge\">\r\n<title>E&#45;&gt;基</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M274.29,-188.42C283.77,-192.18 296.03,-197.04 307.13,-201.44\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"305.91,-204.72 316.5,-205.15 308.49,-198.21 305.91,-204.72\"/>\r\n</g>\r\n<!-- B_ -->\r\n<g id=\"node9\" class=\"node\">\r\n<title>B_</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"337.29\" cy=\"-151\" rx=\"21.4\" ry=\"21.4\"/>\r\n<text text-anchor=\"middle\" x=\"337.29\" y=\"-147.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B_</text>\r\n</g>\r\n<!-- E&#45;&gt;B_ -->\r\n<g id=\"edge13\" class=\"edge\">\r\n<title>E&#45;&gt;B_</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M274.29,-175.57C283.91,-171.76 296.38,-166.82 307.62,-162.36\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"309.06,-165.56 317.07,-158.62 306.48,-159.05 309.06,-165.56\"/>\r\n</g>\r\n<!-- 大 -->\r\n<g id=\"node10\" class=\"node\">\r\n<title>大</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"417.48\" cy=\"-182\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"417.48\" y=\"-178.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">大</text>\r\n</g>\r\n<!-- B_&#45;&gt;大 -->\r\n<g id=\"edge5\" class=\"edge\">\r\n<title>B_&#45;&gt;大</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M357.62,-158.66C366.5,-162.18 377.24,-166.44 387.1,-170.35\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"385.97,-173.67 396.56,-174.1 388.55,-167.16 385.97,-173.67\"/>\r\n</g>\r\n<!-- E_ -->\r\n<g id=\"node11\" class=\"node\">\r\n<title>E_</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"417.48\" cy=\"-121\" rx=\"20.6\" ry=\"20.6\"/>\r\n<text text-anchor=\"middle\" x=\"417.48\" y=\"-117.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">E_</text>\r\n</g>\r\n<!-- B_&#45;&gt;E_ -->\r\n<g id=\"edge14\" class=\"edge\">\r\n<title>B_&#45;&gt;E_</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M357.62,-143.58C366.94,-140.01 378.29,-135.65 388.54,-131.72\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"389.83,-134.97 397.91,-128.12 387.32,-128.44 389.83,-134.97\"/>\r\n</g>\r\n<!-- 学 -->\r\n<g id=\"node12\" class=\"node\">\r\n<title>学</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"497.68\" cy=\"-150\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"497.68\" y=\"-146.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">学</text>\r\n</g>\r\n<!-- E_&#45;&gt;学 -->\r\n<g id=\"edge6\" class=\"edge\">\r\n<title>E_&#45;&gt;学</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M437.41,-128.02C446.36,-131.34 457.25,-135.38 467.24,-139.08\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"466.24,-142.44 476.83,-142.64 468.67,-135.88 466.24,-142.44\"/>\r\n</g>\r\n<!-- S -->\r\n<g id=\"node13\" class=\"node\">\r\n<title>S</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"497.68\" cy=\"-92\" rx=\"18\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"497.68\" y=\"-88.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">S</text>\r\n</g>\r\n<!-- E_&#45;&gt;S -->\r\n<g id=\"edge15\" class=\"edge\">\r\n<title>E_&#45;&gt;S</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M437.41,-113.98C447.52,-110.23 460.1,-105.56 471.06,-101.5\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"472.41,-104.73 480.56,-97.97 469.97,-98.17 472.41,-104.73\"/>\r\n</g>\r\n<!-- 在 -->\r\n<g id=\"node14\" class=\"node\">\r\n<title>在</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"582.42\" cy=\"-126\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"582.42\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">在</text>\r\n</g>\r\n<!-- S&#45;&gt;在 -->\r\n<g id=\"edge7\" class=\"edge\">\r\n<title>S&#45;&gt;在</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M514.63,-98.55C525.34,-102.95 539.74,-108.87 552.44,-114.09\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"551.14,-117.34 561.72,-117.9 553.8,-110.86 551.14,-117.34\"/>\r\n</g>\r\n<!-- _B_ -->\r\n<g id=\"node15\" class=\"node\">\r\n<title>_B_</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"582.42\" cy=\"-59\" rx=\"26.8\" ry=\"26.8\"/>\r\n<text text-anchor=\"middle\" x=\"582.42\" y=\"-55.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_B_</text>\r\n</g>\r\n<!-- S&#45;&gt;_B_ -->\r\n<g id=\"edge16\" class=\"edge\">\r\n<title>S&#45;&gt;_B_</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M514.63,-85.63C524.1,-81.86 536.46,-76.93 547.98,-72.34\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"549.37,-75.55 557.37,-68.59 546.78,-69.05 549.37,-75.55\"/>\r\n</g>\r\n<!-- 芬 -->\r\n<g id=\"node16\" class=\"node\">\r\n<title>芬</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"671.07\" cy=\"-92\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"671.07\" y=\"-88.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">芬</text>\r\n</g>\r\n<!-- _B_&#45;&gt;芬 -->\r\n<g id=\"edge8\" class=\"edge\">\r\n<title>_B_&#45;&gt;芬</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M607.56,-68.19C617.83,-72.1 629.91,-76.7 640.7,-80.81\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"639.56,-84.12 650.15,-84.41 642.05,-77.58 639.56,-84.12\"/>\r\n</g>\r\n<!-- _E_ -->\r\n<g id=\"node17\" class=\"node\">\r\n<title>_E_</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"671.07\" cy=\"-26\" rx=\"26\" ry=\"26\"/>\r\n<text text-anchor=\"middle\" x=\"671.07\" y=\"-22.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">_E_</text>\r\n</g>\r\n<!-- _B_&#45;&gt;_E_ -->\r\n<g id=\"edge17\" class=\"edge\">\r\n<title>_B_&#45;&gt;_E_</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M607.56,-49.81C616.65,-46.34 627.15,-42.34 636.92,-38.62\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"638.26,-41.86 646.36,-35.03 635.77,-35.32 638.26,-41.86\"/>\r\n</g>\r\n<!-- 兰 -->\r\n<g id=\"node18\" class=\"node\">\r\n<title>兰</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"755.16\" cy=\"-26\" rx=\"22.2\" ry=\"22.2\"/>\r\n<text text-anchor=\"middle\" x=\"755.16\" y=\"-22.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">兰</text>\r\n</g>\r\n<!-- _E_&#45;&gt;兰 -->\r\n<g id=\"edge9\" class=\"edge\">\r\n<title>_E_&#45;&gt;兰</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M697.15,-26C705.17,-26 714.16,-26 722.58,-26\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"722.84,-29.5 732.84,-26 722.84,-22.5 722.84,-29.5\"/>\r\n</g>\r\n</g>\r\n</svg>\r\n",
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1f64bcfd4c8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "f = graphviz.Digraph()\n",
    "f.attr(rankdir = 'LR',splines = 'line', size='6,6')\n",
    "f.attr('node', shape='circle')\n",
    "f.edge('B', '赫')\n",
    "f.edge('M', '尔')\n",
    "f.edge('M_', '辛')\n",
    "f.edge('E', '基')\n",
    "f.edge('B_', '大')\n",
    "f.edge('E_', '学')\n",
    "f.edge('S', '在')\n",
    "f.edge('_B_', '芬')\n",
    "f.edge('_E_', '兰')\n",
    "f.edge('B','M')\n",
    "f.edge('M','M_')\n",
    "f.edge('M_','E')\n",
    "f.edge('E','B_')\n",
    "f.edge('B_','E_')\n",
    "f.edge('E_','S')\n",
    "f.edge('S','_B_')\n",
    "f.edge('_B_','_E_')\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61597deb",
   "metadata": {},
   "source": [
    "### So we need to calculate the probability of transferring from one state to another state. Called transition probability. Also the probability of converting from state into observation, named emission probability. Chinese word segmentation actually belongs to decoding task in HMM. That's given transition and emission probabilities as well as an observation sequence, to discover the best hidden states of observation sequence. In addition, we also need initial probability so that we can have the :\n",
    "$S=argmax(P(w_{1})*P(w_{2}|w_{1})*P(w_{3}|w_{2})...*P(w_{i}|w_{i-1}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97273b",
   "metadata": {},
   "source": [
    "### To search the best state combination, viterbi algorithm will also be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5056d2",
   "metadata": {},
   "source": [
    "### The following cells are codes for implementing HMM on Chinese word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a2b2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T21:39:31.510930Z",
     "start_time": "2021-12-23T21:39:31.413325Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'./training_and_testing_data/pku_training.txt',encoding='gbk') as f:# the path may need change\n",
    "        content=f.readlines()\n",
    "content=\"\".join(content).strip()\n",
    "#content=re.sub(\"。|？|！|\\n\",\"<end>\",content)\n",
    "content=content.split(\"\\n\")\n",
    "while '  ' in content:\n",
    "    content.remove('  ')\n",
    "while '' in content:\n",
    "    content.remove('')\n",
    "for i,sentence in enumerate(content):\n",
    "    content[i]=content[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9933406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T21:39:32.164569Z",
     "start_time": "2021-12-23T21:39:32.056685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1109947"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('  '.join(content).split(\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9088769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T21:41:16.401119Z",
     "start_time": "2021-12-23T21:41:15.230247Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class HMM_CWS:\n",
    "    def __init__(self):\n",
    "        self.trans_prob={}\n",
    "        self.emiss_prob={}\n",
    "        self.initi_prob={}\n",
    "        self.state_count={}\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.states=['B','M','E','S']\n",
    "        for state in self.states:\n",
    "            self.trans_prob[state]={}#initialize the transprob\n",
    "            for next_state in self.states:\n",
    "                self.trans_prob[state][next_state]=0 #{'B':{'B':0,'M':0,'E':0,'S':0},'M':...}\n",
    "            self.state_count[state]=0\n",
    "            self.emiss_prob[state]={}\n",
    "            self.initi_prob[state]=0\n",
    "    def observ_to_state(self,word):\n",
    "        if len(word)==1:\n",
    "            return 'S'\n",
    "        if len(word) ==2:\n",
    "            return 'BE'\n",
    "        else:\n",
    "            return 'B%sE'%('M'*(len(word)-2))#otherwise, for example a 4-character-long word should be 'BMME'\n",
    "        \n",
    "    def train_hmm(self,corpus):\n",
    "        self.initialize()\n",
    "        for sentence in corpus:\n",
    "            state_sequence=[]\n",
    "            for index,word in enumerate(sentence.split(\"  \")):\n",
    "                if index ==0:#the word at the beginning should be added into initial_prob\n",
    "                    state_sequence.extend(self.observ_to_state(word))\n",
    "                    self.initi_prob[state_sequence[0]]+=1#add initial state to the initial prob, either B or S\n",
    "                else:\n",
    "                    state_sequence.extend(self.observ_to_state(word))\n",
    "            #transition state\n",
    "            for i,state in enumerate(state_sequence): # for state sequence: 'BEBESSBMMME', start from E, ignoring the first B\n",
    "                self.trans_prob[state_sequence[i-1]][state]+=1#transition prob such as 'B->E'\n",
    "            \n",
    "            for i,char in enumerate(sentence.replace(' ','')):#add count for emission prob\n",
    "                if char in self.emiss_prob[state_sequence[i]]:\n",
    "                    self.emiss_prob[state_sequence[i]][char]+=1\n",
    "                else:\n",
    "                    self.emiss_prob[state_sequence[i]][char]=self.emiss_prob[state_sequence[i]].get(char,0)+1\n",
    "        \n",
    "        #convert frequency into prob\n",
    "        initi_N=sum(self.initi_prob.values())\n",
    "        for state in self.states:#compute the probability of initial_prob\n",
    "            self.initi_prob[state]=-math.log((self.initi_prob[state]+1)/initi_N)# add one smoothing\n",
    "        #calculate transition matrix prob\n",
    "        for state in self.trans_prob:\n",
    "            trans_N=sum(self.trans_prob[state].values())\n",
    "            for next_state in self.trans_prob[state]:\n",
    "                self.trans_prob[state][next_state]=-math.log((self.trans_prob[state][next_state]+1)/trans_N)\n",
    "        #calculate the emission matrix prob\n",
    "        for state in self.emiss_prob:\n",
    "            emiss_N=sum(self.emiss_prob[state].values())\n",
    "            for char in self.emiss_prob[state]:\n",
    "                self.emiss_prob[state][char]=-math.log((self.emiss_prob[state][char]+1)/emiss_N)\n",
    "        #return self.trans_prob,self.initi_prob,self.emiss_prob\n",
    "        \n",
    "    def split_sentence(self,sentence):\n",
    "        pred_state_seq=[]\n",
    "        last_state_prob={'B':0,'M':math.inf,'E':math.inf,'S':0}#since initial state is impossible to be M and E, so they are infinite\n",
    "        ###calculate prob for each hidden state and record each path \n",
    "        for i,char in enumerate(sentence):\n",
    "            if i ==0:#if the character is at the beginning, the hidden state can only be B(begin) or S (single word)\n",
    "                pred_state_seq.append([(state,last_state_prob[state]+self.initi_prob[state]+self.emiss_prob[state].get(char,1)) for state in ['B','S']])\n",
    "                # only B and S can be a state at the first beginning of a sentence, here get(char,1) 1 is default value for unseen observations. It might be a bit problematic.\n",
    "                #pred_state_seq will add a tuple like: ('B', 4.578),('S','6.346')\n",
    "                #There is one issue: how to deal with a unseen character whose emission prob doesn't exist\n",
    "                for state in ['B','S']:\n",
    "                    last_state_prob[state]=self.initi_prob[state]+self.emiss_prob[state].get(char,1)# equals to P(state|start)*P(character|state)\n",
    "                #print(last_state_prob)\n",
    "            else:\n",
    "                current_state_prob=[]\n",
    "                #print(i,char)\n",
    "                for current_state in self.states:\n",
    "                    #print(type(current_state),current_state,last_state_prob)\n",
    "                    current_state_prob.append(min([(former_state,current_state,last_state_prob[former_state]+self.trans_prob[former_state][current_state]+self.emiss_prob[current_state].get(char,1)) for former_state in self.states],key=lambda x:x[-1]))\n",
    "                    #current_state_prob will include a tuple like ('B','E',5.346) where 'E' is current state and its best former state is 'B'\n",
    "                for i,current_state in enumerate(self.states):\n",
    "                    last_state_prob[current_state]=current_state_prob[i][-1]#renew the weight for each state\n",
    "                    #print(current_state_prob)\n",
    "                pred_state_seq.append(current_state_prob)#will record every state at present and its best previous state and prob\n",
    "        #print(last_state_prob)\n",
    "        #print(pred_state_seq)\n",
    "        \n",
    "        best_former_state,best_current_state,prob=min([x for x in pred_state_seq[-1] if x[1] in ['E','S']],key=lambda x:x[-1])\n",
    "        #Only 'E' and 'S' make sense for the last state, so compare the total weight of two paths. Select best last state\n",
    "        final_state=[]\n",
    "        final_state.append(best_current_state)\n",
    "        for i in range(len(pred_state_seq)-2,0,-1):#tracing back, so the index should be from large to small\n",
    "            best_former_state, best_current_state,prob=[x for x in pred_state_seq[i] if x[1]==best_former_state][0]\n",
    "            final_state.append(best_current_state)\n",
    "        final_state.append(best_former_state)\n",
    "        #print(final_state[::-1])\n",
    "        \n",
    "        ###convert states into word segmentations\n",
    "        word_split,start,end=[],0,0\n",
    "        for i,state in enumerate(final_state[::-1]):\n",
    "            #print(i)\n",
    "            if state=='B':\n",
    "                start=i\n",
    "            elif state =='E':\n",
    "                end=i\n",
    "                word_split.append(sentence[start:end+1])#add the segmentation into split list\n",
    "            elif state == 'S':\n",
    "                word_split.append(sentence[i:i+1])\n",
    "        return '  '.join(word_split)\n",
    "                \n",
    "hmm=HMM_CWS()\n",
    "hmm.train_hmm(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc382b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T21:49:14.767304Z",
     "start_time": "2021-12-23T21:49:14.748282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我  的  同学  叫  罗  逸凡'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.split_sentence(\"我的同学叫罗逸凡\")# My classmate whose name is Luo Yifan, and the HMM successfully recognizes a name: 罗（Luo）逸凡（Yifan）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3a338b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:59:07.612166Z",
     "start_time": "2021-12-21T21:59:06.233447Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(r'./training_and_testing_data/pku_test.txt',encoding='gbk') as f:# the path may need change\n",
    "    test=f.readlines()\n",
    "for i,s in enumerate(test):\n",
    "    test[i]=hmm.split_sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6580d44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T19:49:27.771634Z",
     "start_time": "2021-12-21T19:49:27.739185Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('pku_test_segmentation_hmm.txt',\"a\") as f:\n",
    "    f.writelines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9324bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
